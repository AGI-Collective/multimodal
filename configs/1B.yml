# GPT-2 pretraining setup
{
  # parallelism settings ( you will want to change these based on your cluster setup, ideally scheduling pipeline stages
  # across the node boundaries )
  "pipe_parallel_size": 1,
  "model_parallel_size": 1,

  # model settings
  "num-layers": 16,
  "hidden-size": 2048,
  "num-attention-heads": 8,
  "seq-length": 2048,
  "max-position-embeddings": 2048,
  "pos-emb": "rotary",
  "rotary-pct": 0.25,
  "no-weight-tying": true,
  "gpt-j-residual": true,
  "output-layer-parallelism": "column",

  "vision_encoder_args": {
    "arch": "vit_base", 
    "modality": "vision",
    "patch_size": 14,
    "drop_path_rate": 0.4,
    "ffn_layer": "swiglufused",
    "block_chunks": 4,
    "img_size": 518,
    "pretrained_weights": '/p/scratch/ccstdl/gupta6/dinov2/dinobase.pt',
    "freeze_encoder": True,
    "add_lora": False,
    "pretrained": False,
    "encoder_type": "dinov2_base",
    "embed_dropout_prob": 0.1,
    "use_embed_layernorm": True,
    "perceiver_seq_length": 64,
    "num_layers_to_unfreeze": 0,
  },

  "attention-config": [[["flash"], 16]],

  # these should provide some speedup but takes a while to build, set to true if desired
  "scaled_upper_triang_masked_softmax_fusion": true,
  "bias_gelu_fusion": true,

  # init methods
  "init_method": "small_init",
  "output_layer_init_method": "wang_init",

  # optimizer settings
  "optimizer": {
    "type": "Adam",
    "params": {
      "lr": 0.00025,
      "betas": [0.9, 0.95],
      "eps":  1.0e-8,
    }
  },
  "min_lr": 0.000025,

  # "lr_param_groups_config": {
  #   "image_encoder.encoder":{
  #     "decay_style" : "cosine",
  #     "start_lr": 0.0002,
  #     "min_lr": 0.00002,
  #     "warmup_iter": 600,
  #     "end_iter": 5000,
  #   }
  # },

  # for all zero_optimization options, see https://www.deepspeed.ai/docs/config-json/#zero-optimizations-for-fp16-training
  "zero_optimization": {
  "stage": 1,
  "allgather_partitions": True,
  "allgather_bucket_size": 500000000,
  "overlap_comm": True,
  "reduce_scatter": True,
  "reduce_bucket_size": 500000000,
  "contiguous_gradients": True,
  "cpu_offload": false
},

  # batch / data settings
#  "train_batch_size": 32,
  "train_micro_batch_size_per_gpu": 8,
  "gradient_accumulation_steps": 1,
  "data-impl": "mmap",
  "split": "949,50,1",

  # activation checkpointing
  "checkpoint_activations": true,
  "checkpoint_num_layers": 1,
  "partition_activations": true,
  "synchronize_each_layer": true,

  # regularization
  "gradient_clipping": 1.0,
  "weight_decay": 0.1,
  "hidden_dropout": 0,
  "attention_dropout": 0,

  # precision settings
  # "fp16": {
  #   "fp16": true,
  #   "enabled": true,
  #   "loss_scale": 0,
  #   "loss_scale_window": 1000,
  #   "hysteresis": 2,
  #   "min_loss_scale": 1
  # },

  "bf16": {
   "enabled": true
  },
  "precision": "bfloat16",
  "fp32_allreduce": true,

  "data_types": {
     "grad_accum_dtype": "fp32"
   },


  # misc. training settings
  "train_iters": 5000,
  "lr_decay_iters": 5000,
  "distributed_backend": "nccl",
  "lr_decay_style": "cosine",
  "warmup": 0.01,
  "checkpoint_factor": 1000,
  "eval_interval": 100,
  "eval_iters": 10,

  # logging
  "log_interval": 10,
  "steps_per_print": 10,
  "keep_last_n_checkpoints": 4,
  "wall_clock_breakdown": true,
  
#  "launcher": "openmpi",
#  "launcher": "slurm",
#  "deepspeed_slurm": true
#  "deepspeed_mpi": true,
# "launcher": "openmpi"
}
